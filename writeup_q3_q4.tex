\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}

\geometry{a4paper, margin=1in}

\title{CS 440 Assignment 1 - Questions 3 \& 4}
\author{Your Name}
\date{\today}

\begin{document}

\maketitle

\section*{Question 3: Repeated Forward A* vs Repeated Backward A*}

\subsection*{Methodology}
We implemented two variations of the Repeated A* algorithm:
\begin{itemize}
    \item \textbf{Repeated Forward A*}: The agent plans a path from its current position to the goal. The heuristic used is the Manhattan distance from a node $n$ to the goal. 
    \item \textbf{Repeated Backward A*}: The agent plans a path from the goal to its current position. The heuristic used is the Manhattan distance from a node $n$ to the current position of the agent.
\end{itemize}
In both cases, ties in f-values were broken in favor of larger g-values (and then randomly for remaining ties). The agent moves along the planned path until it reaches the goal or encounters a blocked cell, at which point it updates its internal map and re-plans.

\subsection*{Results}
We ran experiments on 50 randomly generated gridworlds (101x101). The table below summarizes the number of cell expansions and runtime for both algorithms.

\begin{center}
\begin{tabular}{|l|r|r|r|r|}
\hline
\textbf{Maze} & \textbf{Fwd Expansions} & \textbf{Bwd Expansions} & \textbf{Fwd Time (s)} & \textbf{Bwd Time (s)} \\
\hline
Maze 1 & 53122 & 867400 & 0.1479 & 2.5587 \\ 
Maze 2 & 155743 & 320669 & 0.3755 & 0.8432 \\
Maze 3 & 652961 & 1506987 & 1.5181 & 3.7980 \\
Maze 4 & 2209490 & 2294901 & 5.5176 & 5.7610 \\
Maze 5 & 181520 & 559091 & 0.4607 & 1.5700 \\
... & ... & ... & ... & ... \\
Maze 50 & 214130 & 1058216 & 0.5461 & 2.5932 \\
\hline
\textbf{Average} & \textbf{224415.9} & \textbf{740341.8} & \textbf{approx 0.5s} & \textbf{approx 1.8s} \\
\hline
\end{tabular}
\end{center}

\subsection*{Analysis}
\textbf{Observation:} Repeated Backward A* consistently expanded significantly more nodes than Repeated Forward A* (approximately 3.3 times more on average). 

\textbf{Reasoning:}
In Repeated A*, the agent performs a new search from scratch whenever it encounters an obstacle. 
\begin{enumerate}
    \item \textbf{Repeated Forward A*}: The search plans from the agent's current position (Start) to the Goal. As the agent moves, it often enters constrained areas (like corridors or rooms) to reach the goal. When replanning is triggered inside such areas, the search is constrained by the local walls and directed towards the exit/goal. This limits the search space fan-out.
    \item \textbf{Repeated Backward A*}: The search plans from the Goal to the agent's current position. When the agent is in a constrained area, the Backward search starts from the Goal (which is typically in a more open or unvisited area) and must "find" the agent. This is effectively searching for a specific target inside a valid tunnel from a large open space, which often results in flooding the open space before entering the correct tunnel. This leads to a much larger number of node expansions.
\end{enumerate}
Tie-breaking favoring larger g-values (closer to the target) was used in both cases, so the performance difference is primarily due to the asymmetry of the search space relative to the start/goal configurations in these gridworlds.

\section*{Question 4: Heuristic Consistency and Adaptive A*}

\subsection*{Part 1: Consistency of Manhattan Distance}
\textbf{Statement:} The Manhattan distance non-overestimating heuristic $h(n)$ is consistent in gridworlds where the agent can move only in the four main compass directions.

\textbf{Proof:}
A heuristic $h(n)$ is consistent if for every node $n$ and every successor $n'$ of $n$ generated by any action $a$, the estimated cost of reaching the goal from $n$ is no greater than the step cost of getting to $n'$ plus the estimated cost of reaching the goal from $n'$:
\[ h(n) \le c(n, n') + h(n') \]
In a gridworld with 4-way movement, the step cost $c(n, n') = 1$ for any adjacent non-blocked cells $n, n'$.
Let $n = (x, y)$ and $goal = (x_g, y_g)$.
The Manhattan distance is $h(n) = |x - x_g| + |y - y_g|$.
Let $n' = (x', y')$ be a neighbor of $n$. Then $|x - x'| + |y - y'| = 1$ since they differ by exactly 1 in one coordinate.
By the Triangle Inequality for the $L_1$ norm (Manhattan distance):
\[ d(n, goal) \le d(n, n') + d(n', goal) \]
Substituting the heuristic values:
\[ h(n) \le 1 + h(n') \]
Since $c(n, n') = 1$, this satisfies the consistency condition $h(n) \le c(n, n') + h(n')$. $\blacksquare$

\subsection*{Part 2: Consistency of Adaptive A* Heuristics}
\textbf{Statement:} Adaptive A* updates the heuristic values using $h_{new}(s) = g(s_{goal}) - g(s)$ for all expanded states $s$. Prove that if the initial heuristic was consistent, the updated heuristic $h_{new}$ remains consistent, even if action costs increase.

\textbf{Proof:}
Let $h_{new}(n)$ be the updated heuristic. We want to show $h_{new}(n) \le c_{new}(n, n') + h_{new}(n')$ for any edge $(n, n')$.
During the search (before cost increases), for any expanded node $n$, $h_{new}(n)$ is defined as the exact cost of the shortest path from $n$ to the goal in the current map (with costs $c_{old}$).
Thus, $h_{new}(n)$ satisfies the triangle inequality perfectly for the old costs:
\[ h_{new}(n) \le c_{old}(n, n') + h_{new}(n') \]
(Equality holds if $n \to n'$ is part of an optimal path).
Now, suppose action costs increase (e.g., an edge becomes blocked, cost $1 \to \infty$).
Let the new costs be $c_{new}(n, n')$. Since costs only increase, we have $c_{new}(n, n') \ge c_{old}(n, n')$.
Therefore:
\[ h_{new}(n) \le c_{old}(n, n') + h_{new}(n') \le c_{new}(n, n') + h_{new}(n') \]
\[ h_{new}(n) \le c_{new}(n, n') + h_{new}(n') \]
Thus, the consistency condition holds for the updated heuristic values with respect to the new (increased) edge costs. $\blacksquare$

\end{document}
